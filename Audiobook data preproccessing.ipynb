{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af33ac47",
   "metadata": {},
   "source": [
    "# Audiobooks business case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a66f3f",
   "metadata": {},
   "source": [
    "## Preprocess the data. Balance the dataset. Create 3 datasets: training, validation, and test. Save the newly created sets in a tensor friendly format (e.g. *.npz)\n",
    "\n",
    "Since we are dealing with real life data, we will need to preprocess it a bit. This is the relevant code, which is not that hard, but is crucial to creating a good model.\n",
    "\n",
    "If you want to know how to do that, go through the code. In any case, this should do the trick for most datasets organized in the way: many inputs, and then 1 cell containing the targets (supervised learning datasets). Keep in mind that a specific problem may require additional preprocessing.\n",
    "\n",
    "Note that we have removed the header row, which contains the names of the categories. We simply want the data.\n",
    "\n",
    "This code does not include comments - it is the same as the one in the lesson. Please refer to the other file if you want the code with comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e78757e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47f39d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = np.loadtxt('Audiobooks-data.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d18a0462",
   "metadata": {},
   "outputs": [],
   "source": [
    "unscaled_inputs = raw_data[:,1:-1]\n",
    "unscaled_targets = raw_data[:,-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0dd350a",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_ones = int(np.sum(unscaled_targets))\n",
    "no_of_zeros = 0\n",
    "indices_to_remove = []\n",
    "for i in range(unscaled_targets.shape[0]):\n",
    "    if unscaled_targets[i] == 0 :\n",
    "        no_of_zeros += 1\n",
    "        if no_of_zeros > no_of_ones :\n",
    "            indices_to_remove.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dab077d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "unscaled_inputs = np.delete(unscaled_inputs,indices_to_remove,axis=0)\n",
    "unscaled_targets = np.delete(unscaled_targets,indices_to_remove,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b6ab98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = StandardScaler()\n",
    "scale.fit(unscaled_inputs)\n",
    "scaled_inputs = scale.transform(unscaled_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d7ee81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_scaled_indices = np.arange(scaled_inputs.shape[0])\n",
    "np.random.shuffle(shuffled_scaled_indices)\n",
    "shuffled_inputs = scaled_inputs[shuffled_scaled_indices]\n",
    "shuffled_targets = unscaled_targets[shuffled_scaled_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d27fdfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1804.0 3579 0.5040514110086617\n",
      "203.0 447 4.03579418344519\n",
      "230.0 448 0.5133928571428571\n"
     ]
    }
   ],
   "source": [
    "sample_count = shuffled_inputs.shape[0]\n",
    "train_data_count = int(0.8*sample_count)\n",
    "validation_data_count = int(0.1*sample_count)\n",
    "test_data_count = sample_count - (train_data_count + validation_data_count)\n",
    "\n",
    "train_sample_input_data = shuffled_inputs[:train_data_count]\n",
    "train_sample_tartget_data = shuffled_targets[:train_data_count]\n",
    "\n",
    "validation_sample_inputs_data = shuffled_inputs[train_data_count: train_data_count + validation_data_count]\n",
    "validation_sample_target_data = shuffled_targets[train_data_count: train_data_count + validation_data_count]\n",
    "\n",
    "test_sample_inputs_data = shuffled_inputs[train_data_count + validation_data_count : ]\n",
    "test_sample_target_data = shuffled_targets[train_data_count + validation_data_count : ]\n",
    "\n",
    "print(np.sum(train_sample_tartget_data), train_data_count,np.sum(train_sample_tartget_data)/train_data_count)\n",
    "print(np.sum(validation_sample_target_data),validation_data_count,np.sum(train_sample_tartget_data)/validation_data_count)\n",
    "print(np.sum(test_sample_target_data),test_data_count,np.sum(test_sample_target_data)/test_data_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19524f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('Audiobook_train_data',inputs = train_sample_input_data , targets = train_sample_tartget_data)\n",
    "np.savez('Audiobook_validation_data',inputs = validation_sample_inputs_data , targets = validation_sample_target_data)\n",
    "np.savez('Audiobook_test_data',inputs = test_sample_inputs_data , targets = test_sample_target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb956ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
